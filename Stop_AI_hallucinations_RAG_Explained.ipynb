{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**RAG-- Retrival Agumented Generation**\n",
        "\n",
        "\n",
        "What is RAG or Retrival Agumented generation ?\n",
        "\n",
        "Ans : If there is Sensitive internal  Data of a Company in the form of \"PDF's,in DB's,and Documents\" and if you wnat to retrive any information thi private documents..an LLM cannot help in this and give an appropriate answers as these are internal documents and thhis information does not exists on a Public Platform like LLM's.. There comes the conceptof RAG.\n",
        "\n",
        "Fine Tunning of teh LLM's are done for the Internal documents ..This is called as RAG (Retrival Agumneted Generation).\n",
        "\n",
        "**RAG is the process of optimizing the Output of LLM's**\n",
        "\n",
        "**RAG has two steps involved **\n",
        "1. Ingestion/Indexing\n",
        "\n",
        "2. Retreival\n",
        "\n",
        "\n",
        "1. Indexing --\n",
        "\n",
        "\n",
        " a.Consider an internal PDF =====> Break this PDF into multiple chunks ====> Each of these chunks contains textual data. This data is data is converted into Vectors.\n",
        "\n",
        " b.These vectors are stored in the \"Vector Database. This database is used to store the vector embeddings of the original PDF.\n",
        " Example of a Vector DB is Quadrant DB(This DB is quite Fast) .\n",
        "\n",
        "\n",
        " The whole Processof breaking the PDF into chunksand converting chunks into Vectors and storing these Vectors in a Database is called as **Indexing orIngestion**\n",
        "\n",
        "\n",
        " Indexing Explaination:\n",
        "\n",
        " Knowledge base(PDF or any type of Internal document. \"Ace the Data science Interview\") =====> Loader is used to Load the PDF(Converted into a document) =====> Splitter is used to split the document into chunks or Document Snippets====>Machine Emdeddings are used to convert these Documents  into Embeddings or Vectors using ======> These vectors are embeddings are stored in the Vector Database.\n",
        "\n",
        "\n",
        " 2. Retrieval---\n",
        "\n",
        " Query is written related to the Ingestion as below\n",
        " \" What is machine Learning?\"\n",
        "\n",
        " Query ===> Convert the query into Vectors into Vector Embeddings.\n",
        " During the Indexing orIngestion stp the PDF is converted into Vectors and stored in Vector DB +LLM's are present..\n",
        " Query embeddings is compared (Similarity search) is done and closest answer based on the distance is retrieved =====>Closest answer given to the User.\n",
        "\n",
        "\n",
        "**NOTE --RESULTS FOR A QUERY CAN BE OBTAINED ONLY FROM THE VECTOR DATABASE AS WELL,BUT WITH THE ASSITANCE OF LLM'S ALONG WITH VECTOR DB..WE CAN OBTAIN BETTER RESULTS**\n",
        "\n",
        "\n",
        "** LANGCHAIN MODEL IS USED TO IMPLEMENT THIS CONCEPT OF RAG**\n",
        "\n",
        "\n",
        "** EXAMPLE OF using RAG + LLM's : \"how much annaual leave do i have?\"\n",
        "\n",
        "Soln: This uses the internal document (Called as Knowledge Base) within the Orgainization that is in the Vector Database and also LLM's to give an appropriate answer.\n",
        "\n",
        "\n",
        "\n",
        "**Understanding of Vector Database**\n",
        "\n",
        "For every Vector embeddings there is a unique ID associated with it called as UUID.\n",
        "\n",
        "Structure of a VectorDatabase:\n",
        "\n",
        "\"id\" \"dimensions\" \"payload\"(Payload is a Meta Data,that gives information about the data..)\n",
        "\n",
        "There can be 1024 Dimensions or Features\n",
        "\n",
        "Quadrant Vector database is one of the fastest Database.The response is seen with minium Latency.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0lI_u3f_0rb_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RudnVCErEZA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfqZzpAFrE3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hObGJvPFrE5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZbewEflsrE8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4hfxJ62rE_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ng1WNqAZrFCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eqj8JFf9rFFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n5qnZrT8rFH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zxaXMLBIrFKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-sc_rj2NrFNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lrtI28GMrFQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNxnX_ayrFTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5TeqoqaKrFV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wkDNhQEYrFYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_7xq7TgarFcH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}